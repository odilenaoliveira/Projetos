# -*- coding: utf-8 -*-
"""doenca_cardiaca.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mQoSWb1xMQM8Ymq_Vu6hWgE88XSENRbN
"""



# Importando Dados
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier,ExtraTreesClassifier
from sklearn.metrics import confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler


def load_data(filename):
  dados = pd.read_csv(filename)
  print('Tamanho de Dados:', dados.shape)
  print('Dados Duplicados:', dados.duplicated().sum())
  tabela = pd.DataFrame({
      'Unique':dados.nunique(),
      'Null':dados.isna().sum(),
      'NullPercent':dados.isna().sum() / len(dados),
      'Type':dados.dtypes.values
  })
  display(tabela)
  display(dados.head())

  return dados


def transform_data(dados):
  # Eliminando dados duplicados
  dados.drop_duplicates(inplace=True)
  # Copiando o dados original para uma nova variável
  df = dados.copy()
  # Selecionando apenas dados categóricos para transformação de texto
  cat = df.select_dtypes(include='O')
  # transformando todos os textos em minúsculo
  for c in cat:
    if c != 'Age.Group':
      df[c] = df[c].apply(lambda text: text.lower())

  # substituindo as palavras
  df.replace({'yes':1, 'no':0, 'free':1, 'paid':0, 'married':1, 'single':0, 'female':1, 'male':0, 'urban':1, 'rural':0}, inplace=True)
  # atualizando e transformando os valores categóricos em colunas
  cat = df.select_dtypes(include='O')

  df = pd.get_dummies(df, columns=[col for col in df.columns if col in cat])
  return df


def split_data(df):

  # separando os dados para o treinamento
  y = df.Mortality
  x = df.drop(['Mortality'],axis=1)

  ## Balanceando Dados
  under = RandomUnderSampler()
  over = RandomOverSampler()

  pipeline = Pipeline(steps=[('o',over),('u', under)])
  x, y = pipeline.fit_resample(x, y)

  return x,y


def train_data(x,y,testSize, num):
  x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=testSize, random_state=num)

  scaler = MinMaxScaler()
  x_train = scaler.fit_transform(x_train)
  x_test = scaler.fit_transform(x_test)
  return x_train, x_test, y_train, y_test

def model_data(x_train, x_test, y_train, y_test):
  model = GradientBoostingClassifier(max_depth=7, n_estimators=200, max_features=4, random_state=42)
  model.fit(x_train, y_train)

  y_pred_tg = model.predict(x_train)
  y_pred_g = model.predict(x_test)

  print('Train Score:', model.score(x_train, y_train))
  print('Test Score:',model.score(x_test, y_test))

  return model, x_test, y_test

def visual_data(model,x_test, y_test):
  cm_grade = confusion_matrix(y_test, model.predict(x_test))
  cm_norm_grade = cm_grade / cm_grade.sum(axis=1)

  fig, axes = plt.subplots(1,2,figsize=(14,7))
  sns.heatmap(cm_grade, annot=True, ax=axes[0])
  axes[0].set_title('Valor da Previsão')
  sns.heatmap(cm_norm_grade, annot=True, ax=axes[1])
  axes[1].set_title('Porcentagem da Previsão')
  plt.tight_layout()

  return plt.show()

def runModel(filename):
  dados = load_data(filename)
  df = transform_data(dados)
  x,y = split_data(df)
  x_train, x_test, y_train, y_test = train_data(x,y,0.2, 42)
  model,y_test, x_test = model_data(x_train, x_test, y_train, y_test)
  visual_data(model,y_test, x_test)

if __name__ == '__main__':
  runModel('/content/FIC.Full CSV.csv')